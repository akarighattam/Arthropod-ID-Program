{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arthropod_Identification_code.ipynb",
      "provenance": [],
      "mount_file_id": "1XqIJNkx20ZVydHK8Umi5GBINUQh8GS4_",
      "authorship_tag": "ABX9TyMgcjnJwuDjdFJw+Jw575z6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akarighattam/akarighattam/blob/main/Arthropod_Identification_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Arthropod Dataset"
      ],
      "metadata": {
        "id": "v_nUss9IBjb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install and import fastbook\n",
        "!pip install fastai==2.0.15\n",
        "!pip install fastai2==0.0.30\n",
        "!pip install fastcore==1.0.16\n",
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "from fastbook import *\n",
        "\n",
        "#import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image, ImageOps\n",
        "#mount google drive to access images for dataset\n",
        "from google.colab import drive\n",
        "%matplotlib inline\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0ppEelrWN9wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resize photos of spiders for training and append them to a list\n",
        "train_spiders_folder = '/content/drive/Shareddrives/Image Dataset for Aranya/Arthropod Photos/Train Dataset/Spiders'\n",
        "train_spiders_resized = '/content/drive/Shareddrives/Image Dataset for Aranya/Resized Arthropod Photos/Train Dataset/Spiders'\n",
        "\n",
        "train_list_spiders = []\n",
        "train_labels_spiders = []\n",
        "\n",
        "for img in os.listdir(train_spiders_folder):\n",
        "  #resize_photo = Image.open(os.path.join(train_spiders_folder, img))\n",
        "  #resize_photo = ImageOps.pad(resize_photo, (300, 300), color='black')\n",
        "  #resize_photo.save(f\"{train_spiders_resized}/{img}\")\n",
        "  training_photos = mpimg.imread(os.path.join(train_spiders_resized, img))\n",
        "  train_list_spiders.append([training_photos])\n",
        "  train_labels_spiders.append(\"Spiders\")"
      ],
      "metadata": {
        "id": "zCJZYyF81bwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resize photos of moths for training and append them to a list\n",
        "train_moths_folder = '/content/drive/Shareddrives/Image Dataset for Aranya/Arthropod Photos/Train Dataset/Moths'\n",
        "train_moths_resized = '/content/drive/Shareddrives/Image Dataset for Aranya/Resized Arthropod Photos/Train Dataset/Moths'\n",
        "\n",
        "train_list_moths = []\n",
        "train_labels_moths = []\n",
        "\n",
        "for img in os.listdir(train_moths_folder):\n",
        "  #resize_photo = Image.open(os.path.join(train_moths_folder, img))\n",
        "  #resize_photo = ImageOps.pad(resize_photo, (300, 300), color='black')\n",
        "  #resize_photo.save(f\"{train_moths_resized}/{img}\")\n",
        "  training_photos = mpimg.imread(os.path.join(train_moths_resized, img))\n",
        "  train_list_moths.append([training_photos])\n",
        "  train_labels_moths.append(\"Moths\")"
      ],
      "metadata": {
        "id": "4TqsB_Et3dQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resize photos of spiders for validation and append them to a list\n",
        "valid_spiders_folder = '/content/drive/Shareddrives/Image Dataset for Aranya/Arthropod Photos/Validation Dataset/Spiders'\n",
        "valid_spiders_resized = '/content/drive/Shareddrives/Image Dataset for Aranya/Resized Arthropod Photos/Validation Dataset/Spiders'\n",
        "\n",
        "valid_list_spiders = []\n",
        "valid_labels_spiders = []\n",
        "\n",
        "for img in os.listdir(valid_spiders_folder):\n",
        "  #resize_photo = Image.open(os.path.join(valid_spiders_folder, img))\n",
        "  #resize_photo = ImageOps.pad(resize_photo, (300, 300), color='black')\n",
        "  #resize_photo.save(f\"{valid_spiders_resized}/{img}\")\n",
        "  training_photos = mpimg.imread(os.path.join(valid_spiders_resized, img))\n",
        "  valid_list_spiders.append([training_photos])\n",
        "  valid_labels_spiders.append(\"Spiders\")"
      ],
      "metadata": {
        "id": "qNKXcKgw4iZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resize photos of moths for validation and append them to a list\n",
        "valid_moths_folder = '/content/drive/Shareddrives/Image Dataset for Aranya/Arthropod Photos/Validation Dataset/Moths'\n",
        "valid_moths_resized = '/content/drive/Shareddrives/Image Dataset for Aranya/Resized Arthropod Photos/Validation Dataset/Moths'\n",
        "\n",
        "valid_list_moths = []\n",
        "valid_labels_moths = []\n",
        "\n",
        "for img in os.listdir(valid_moths_folder):\n",
        "  #resize_photo = Image.open(os.path.join(valid_moths_folder, img))\n",
        "  #resize_photo = ImageOps.pad(resize_photo, (300, 300), color='black')\n",
        "  #resize_photo.save(f\"{valid_moths_resized}/{img}\")\n",
        "  training_photos = mpimg.imread(os.path.join(valid_moths_resized, img))\n",
        "  valid_list_moths.append([training_photos])\n",
        "  valid_labels_moths.append(\"Moths\")"
      ],
      "metadata": {
        "id": "B2EwHRR45VBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert each list to a numpy array and then to a float\n",
        "spiders_array_train = np.array(train_list_spiders).astype(np.float32)\n",
        "moths_array_train = np.array(train_list_moths).astype(np.float32)\n",
        "spiders_array_valid = np.array(valid_list_spiders).astype(np.float32)\n",
        "moths_array_valid = np.array(valid_list_moths).astype(np.float32)\n",
        "\n",
        "#convert the numpy arrays to tensors\n",
        "spiders_train = tensor(spiders_array_train)\n",
        "moths_train = tensor(moths_array_train)\n",
        "spiders_valid = tensor(spiders_array_valid)\n",
        "moths_valid = tensor(moths_array_valid)\n",
        "\n",
        "#concatenate the train tensors and define it as train_x (mulitiply 2nd parameter in .view by 3 for rgb)\n",
        "train_x = torch.cat([spiders_train, moths_train]).view(-1, 300*300*3)\n",
        "\n",
        "#define train_y (list of classification labels)\n",
        "train_y = tensor([1]*train_labels_spiders.count(\"Spiders\") + [0]*train_labels_moths.count(\"Moths\")).unsqueeze(1)\n",
        "\n",
        "#concatenate the validation tensors and define it as valid_x (mulitiply 2nd parameter in .view by 3 for rgb)\n",
        "valid_x = torch.cat([spiders_valid, moths_valid]).view(-1, 300*300*3)\n",
        "\n",
        "#define valid_y (list of classification labels)\n",
        "valid_y = tensor([1]*valid_labels_spiders.count(\"Spiders\") + [0]*valid_labels_moths.count(\"Moths\")).unsqueeze(1)"
      ],
      "metadata": {
        "id": "mrm-i4rW6BFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check that the dimensions are correct\n",
        "train_x.shape, train_y.shape, valid_x.shape, valid_y.shape"
      ],
      "metadata": {
        "id": "lZoXEng-VgZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training dataset list containing sublists with (image, label)\n",
        "training_dataset = list(zip(train_x, train_y))\n",
        "x, y = training_dataset[0]\n",
        "\n",
        "#validation dataset list containing sublists with (image, label)\n",
        "validation_dataset = list(zip(valid_x, valid_y))"
      ],
      "metadata": {
        "id": "OokBiPQnj8yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define parameters for initializing random weights\n",
        "def initialize(size, variance = 1.0):\n",
        "  return (torch.randn(size)*variance).requires_grad_()\n",
        "\n",
        "#initialize random weights (mulitiply 1st parameter in of initialize by 3 for rgb)\n",
        "weights = initialize((300*300*3, 1))\n",
        "#initialize bias\n",
        "bias = initialize(1)"
      ],
      "metadata": {
        "id": "sBFZVLcumvVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define loss function (this loss function only works for 2 classifications)\n",
        "def loss_function(predictions, actual_values):\n",
        "  predictions = predictions.sigmoid()\n",
        "  return torch.where(actual_values==1, 1-predictions, predictions).mean()"
      ],
      "metadata": {
        "id": "FkuQcsaIoR8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define DataLoader for training dataset\n",
        "training_dataloader = DataLoader(training_dataset, batch_size = 25, shuffle=True)\n",
        "x_batch, y_batch = first(training_dataloader)\n",
        "\n",
        "#define DataLoader for validation dataset\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size = 25, shuffle=True)"
      ],
      "metadata": {
        "id": "tbQOB_Sm-Deu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function to calculate the gradient\n",
        "def calculate_gradient(x_batch, y_batch, model):\n",
        "  predict_x = model(x_batch)\n",
        "  loss = loss_function(predict_x, y_batch)\n",
        "  loss.backward()"
      ],
      "metadata": {
        "id": "RDsJfPrzICdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define optimizer that updates the weights\n",
        "class OptimizerClass:\n",
        "  def __init__(self, parameters, learning_rate):\n",
        "    self.parameters, self.learning_rate = list(parameters), learning_rate\n",
        "  def step(self, *args, **kwargs):\n",
        "    for p in self.parameters:\n",
        "      p.data -= p.grad.data*self.learning_rate\n",
        "  def zero_gradient(self, *args, **kwargs):\n",
        "    for p in self.parameters:\n",
        "      p.grad.zero_()"
      ],
      "metadata": {
        "id": "Z7ISxWpwNDbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that trains the model for one epoch\n",
        "def train_for_epoch(model):\n",
        "  for x_batch, y_batch in training_dataloader:\n",
        "    calculate_gradient(x_batch, y_batch, model)\n",
        "    optimizer = OptimizerClass(parameters, learning_rate)\n",
        "    optimizer.step()\n",
        "    optimizer.zero_gradient()"
      ],
      "metadata": {
        "id": "1agXnTUecEpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define learning rate and parameters variable\n",
        "learning_rate = 100.\n",
        "parameters = [weights, bias]"
      ],
      "metadata": {
        "id": "RsgJVC85edFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to check accuracy between training batches\n",
        "def batch_accuracy(x_batch, y_batch):\n",
        "  predictions = x_batch.sigmoid()\n",
        "  correct = (predictions>0.5) == y_batch\n",
        "  return correct.float()"
      ],
      "metadata": {
        "id": "IBNBQqOzd2te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that validates the model for one epoch\n",
        "def validate_for_epoch(model):\n",
        "  accuracy = [batch_accuracy(model(x_batch), y_batch) for x_batch, y_batch in validation_dataloader]\n",
        "  return round(torch.stack(accuracy).mean().item(), 4)"
      ],
      "metadata": {
        "id": "VpUHtSm0fYhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that trains the model\n",
        "def train_model(model, epochs):\n",
        "  for i in range(epochs):\n",
        "    train_for_epoch(model)\n",
        "    print(validate_for_epoch(model), end = ' ')"
      ],
      "metadata": {
        "id": "Y3cef3eBZewt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define prediction function\n",
        "def predict(x_batch):\n",
        "  return (x_batch@weights + bias)/270000"
      ],
      "metadata": {
        "id": "pSPN_vvpRSQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The output is the percent of how accurate the model is at predicting the classification correctly\n",
        "print(\"Prediction:\")\n",
        "train_model(predict, 401)\n"
      ],
      "metadata": {
        "id": "625s6Nu4jMcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function that prints the predicted classification and the confidence in percent\n",
        "def identify(path):\n",
        "  print(\"Spider = 1, Moth = 0\")\n",
        "  image = PILImage(PILImage.create(path))\n",
        "  image.show()\n",
        "  image = ImageOps.pad(image, (300, 300), color='black')\n",
        "  pil_image_to_numpy = np.array(image).astype(np.float32)\n",
        "  numpy_to_tensor = tensor(pil_image_to_numpy).view(-1, 300*300*3)\n",
        "  tensor_value = round((predict(numpy_to_tensor).sigmoid()).item(), 4)\n",
        "  if tensor_value>0.7:\n",
        "    print(\"Prediction value:\", tensor_value)\n",
        "    print(\"Spider:\", (str(int(round(tensor_value, 2)*100)))+\"%\", \"likely\")\n",
        "  elif tensor_value<0.3:\n",
        "    print(\"Prediction value:\", tensor_value)\n",
        "    print(\"Moth:\", (str(int(round(1-tensor_value, 2)*100)))+\"%\", \"likely\")\n",
        "  else:\n",
        "    print(\"Prediction value:\", tensor_value)\n",
        "    print(\"Cannot detect any arthropods in the image.\")\n",
        "\n",
        "orbweaver = '/content/drive/MyDrive/Arthropod Identification App - Test Photos/Western Spotted Orbweaver (Neoscona oaxacensis)-1.jpg'\n",
        "idia_moth = '/content/drive/MyDrive/Arthropod Identification App - Test Photos/American Idia Moth.jpg'\n",
        "jumping_spider = '/content/drive/MyDrive/Arthropod Identification App - Test Photos/Flea Jumping Spider.jpg'\n",
        "carpet_moth = '/content/drive/MyDrive/Arthropod Identification App - Test Photos/Bent-line Carpet Moth.jpg'\n",
        "\n",
        "identify(idia_moth)\n"
      ],
      "metadata": {
        "id": "RNna5PeveOrs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}